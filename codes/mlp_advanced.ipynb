{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dl_utlils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dl_utlils.py\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "def relu(x):\n",
    "    return T.maximum(0, x)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return T.nnet.sigmoid(x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return T.tanh(x)\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def negative_log_likelihood(self):\n",
    "        self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
    "        return -T.mean(T.log(self.prob_of_y_given_x)[T.arange(self.y.shape[0]), self.y])\n",
    "\n",
    "    def cross_entropy(self):\n",
    "        self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
    "        return T.mean(T.nnet.categorical_crossentropy(self.prob_of_y_given_x, self.y))\n",
    "\n",
    "    def mean_squared_error(self):\n",
    "        return T.mean((self.x - self.y) ** 2)\n",
    "\n",
    "    def errors(self):\n",
    "        if self.y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError('y should have the same shape as self.y_pred',\n",
    "                            ('y', self.y.type, 'y_pred', self.y_pred.type))\n",
    "\n",
    "        if self.y.dtype.startswith('int'):\n",
    "            self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
    "            self.y_pred = T.argmax(self.prob_of_y_given_x, axis=1)\n",
    "            return T.mean(T.neq(self.y_pred, self.y))\n",
    "        else:\n",
    "            return NotImplementedError()\n",
    "\n",
    "    def accuracy(self):\n",
    "        if self.y.dtype.startswith('int'):\n",
    "            self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
    "            self.y_pred = T.argmax(self.prob_of_y_given_x, axis=1)\n",
    "            return T.mean(T.eq(self.y_pred, self.y))\n",
    "        else:\n",
    "            return NotImplementedError()\n",
    "\n",
    "\n",
    "def shared_data(x, y):\n",
    "    shared_x = theano.shared(\n",
    "        np.asarray(x, dtype=theano.config.floatX), borrow=True)\n",
    "    if y is None:\n",
    "        return shared_x\n",
    "\n",
    "    shared_y = theano.shared(\n",
    "        np.asarray(y, dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "    return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "\n",
    "def build_shared_zeros(shape, name):\n",
    "    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n",
    "    return theano.shared(\n",
    "        value=np.zeros(shape, dtype=theano.config.floatX),\n",
    "        name=name,\n",
    "        borrow=True\n",
    "    )\n",
    "\n",
    "\n",
    "def dropout(x, train, p=0.5, rng = np.random.RandomState(1234)):\n",
    "    masked_x = None\n",
    "    if p > 0.0 and p < 1.0:\n",
    "        seed = rng.randint(2 ** 30)\n",
    "        srng = T.shared_randomstreams.RandomStreams(seed)\n",
    "        mask = srng.binomial(\n",
    "            n=1,\n",
    "            p=1.0 - p,\n",
    "            size=x.shape,\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "        masked_x = x * mask\n",
    "    else:\n",
    "        masked_x = x\n",
    "    return T.switch(T.neq(train, 0), masked_x, x * (1.0 - p))\n",
    "\n",
    "\n",
    "class Optimizer(object):\n",
    "\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            return NotImplementedError()\n",
    "        self.params = params\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        if loss is None:\n",
    "            return NotImplementedError()\n",
    "\n",
    "        self.updates = OrderedDict()\n",
    "        self.gparams = [T.grad(loss, param) for param in self.params]\n",
    "\n",
    "\n",
    "def build_shared_zeros(shape, name):\n",
    "    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n",
    "    return theano.shared(\n",
    "        value=np.zeros(shape, dtype=theano.config.floatX),\n",
    "        name=name,\n",
    "        borrow=True\n",
    "    )\n",
    "\n",
    "\n",
    "class RMSprop(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, alpha=0.99, eps=1e-8, params=None):\n",
    "        super(RMSprop, self).__init__(params=params)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "\n",
    "        self.mss = [\n",
    "            build_shared_zeros(t.shape.eval(), 'ms') for t in self.params]\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        super(RMSprop, self).updates(loss=loss)\n",
    "\n",
    "        for ms, param, gparam in zip(self.mss, self.params, self.gparams):\n",
    "            _ms = ms * self.alpha\n",
    "            _ms += (1 - self.alpha) * gparam * gparam\n",
    "            self.updates[ms] = _ms\n",
    "            self.updates[param] = param - self.learning_rate * \\\n",
    "                gparam / T.sqrt(_ms + self.eps)\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "class AdaDelta(Optimizer):\n",
    "\n",
    "    def __init__(self, rho=0.95, eps=1e-6, params=None):\n",
    "        super(AdaDelta, self).__init__(params=params)\n",
    "\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "        self.accugrads = [\n",
    "            build_shared_zeros(t.shape.eval(), 'accugrad') for t in self.params]\n",
    "        self.accudeltas = [\n",
    "            build_shared_zeros(t.shape.eval(), 'accudelta') for t in self.params]\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        super(AdaDelta, self).updates(loss=loss)\n",
    "\n",
    "        for accugrad, accudelta, param, gparam\\\n",
    "                in zip(self.accugrads, self.accudeltas, self.params, self.gparams):\n",
    "            agrad = self.rho * accugrad + (1 - self.rho) * gparam * gparam\n",
    "            dx = - T.sqrt((accudelta + self.eps) / (agrad + self.eps)) * gparam\n",
    "            self.updates[accudelta] = (\n",
    "                self.rho * accudelta + (1 - self.rho) * dx * dx)\n",
    "            self.updates[param] = param + dx\n",
    "            self.updates[accugrad] = agrad\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "class MomentumSGD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9, params=None):\n",
    "        super(MomentumSGD, self).__init__(params=params)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.vs = [build_shared_zeros(t.shape.eval(), 'v')\n",
    "                   for t in self.params]\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        super(MomentumSGD, self).updates(loss=loss)\n",
    "\n",
    "        for v, param, gparam in zip(self.vs, self.params, self.gparams):\n",
    "            _v = v * self.momentum\n",
    "            _v = _v - self.learning_rate * gparam\n",
    "            self.updates[param] = param + _v\n",
    "            self.updates[v] = _v\n",
    "\n",
    "        return self.updates    \n",
    "\n",
    "class Adam(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gamma=1 - 1e-8, params=None):\n",
    "        super(Adam, self).__init__(params=params)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.b1 = beta1\n",
    "        self.b2 = beta2\n",
    "        self.gamma = gamma\n",
    "        self.t = theano.shared(np.float32(1))\n",
    "        self.eps = eps\n",
    "\n",
    "        self.ms = [build_shared_zeros(t.shape.eval(), 'm')\n",
    "                   for t in self.params]\n",
    "        self.vs = [build_shared_zeros(t.shape.eval(), 'v')\n",
    "                   for t in self.params]\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        super(Adam, self).updates(loss=loss)\n",
    "        self.b1_t = self.b1 * self.gamma ** (self.t - 1)\n",
    "\n",
    "        for m, v, param, gparam \\\n",
    "                in zip(self.ms, self.vs, self.params, self.gparams):\n",
    "            _m = self.b1_t * m + (1 - self.b1_t) * gparam\n",
    "            _v = self.b2 * v + (1 - self.b2) * gparam ** 2\n",
    "\n",
    "            m_hat = _m / (1 - self.b1 ** self.t)\n",
    "            v_hat = _v / (1 - self.b2 ** self.t)\n",
    "\n",
    "            self.updates[param] = param - self.alpha * \\\n",
    "                m_hat / (T.sqrt(v_hat) + self.eps)\n",
    "            self.updates[m] = _m\n",
    "            self.updates[v] = _v\n",
    "        self.updates[self.t] = self.t + 1.0\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "# Multi Layer Perceptron\n",
    "\n",
    "class Layer:\n",
    "    # Constructor\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        rng = np.random.RandomState(1234)\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.W = theano.shared(rng.uniform(low=-0.08, high=0.08,\n",
    "                                           size=(in_dim, out_dim)\n",
    "                                           ).astype('float32'), name='W')\n",
    "        self.b = theano.shared(np.zeros(out_dim).astype('float32'), name='b')\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "\n",
    "    # Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        self.z = T.dot(x, self.W) + self.b\n",
    "        return self.z\n",
    "\n",
    "class Activation:\n",
    "    # Constructor\n",
    "    def __init__(self, function):\n",
    "        self.function = function\n",
    "        self.params = []\n",
    "\n",
    "    # Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        self.z = self.function(x)\n",
    "        return self.z\n",
    "    \n",
    "class BatchNorm:\n",
    "    # Constructor\n",
    "    def __init__(self, shape, epsilon=np.float32(1e-5)):\n",
    "        self.shape = shape\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.gamma = theano.shared(np.ones(self.shape, dtype=\"float32\"),\n",
    "                                   name=\"gamma\")\n",
    "        self.beta = theano.shared(np.zeros(self.shape, dtype=\"float32\"),\n",
    "                                  name=\"beta\")\n",
    "        self.params = [self.gamma, self.beta]\n",
    "\n",
    "    # Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        if x.ndim == 2:\n",
    "            mean = T.mean(x, axis=0, keepdims=True)\n",
    "            std = T.sqrt(T.var(x, axis=0, keepdims=True) + self.epsilon)\n",
    "        elif x.ndim == 4:\n",
    "            mean = T.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "            std = T.sqrt(T.var(x, axis=(0, 2, 3), keepdims=True) +\n",
    "                         self.epsilon)\n",
    "\n",
    "        normalized_x = (x - mean) / std\n",
    "        self.z = self.gamma * normalized_x + self.beta\n",
    "        return self.z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/anaconda/envs/pure_theano/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 775M (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                           mnist.target.astype('int32'),\n",
    "                           random_state=42)\n",
    "\n",
    "mnist_X = mnist_X / 255.0\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os \n",
    "base_folder = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np.eye(10)[train_y].astype('int32')\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-423442000ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m           (epoch + 1, valid_cost,\n\u001b[1;32m     50\u001b[0m            f1_score(np.argmax(valid_y, axis=1).astype('int32'),\n\u001b[0;32m---> 51\u001b[0;31m                     pred_y, average='macro')))\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from dl_utlils import *\n",
    "activation = relu\n",
    "mlp_layers =  [784, 500, 500, 500, 10]\n",
    "layers = []\n",
    "for i_layer in range(len(mlp_layers)-2):\n",
    "    layers.append(Layer(mlp_layers[i_layer], mlp_layers[i_layer+1]))\n",
    "    # BatchNorm(mlp_layers[i_layer+1],mlp_layers[i_layer+1])\n",
    "    layers.append(Activation(relu))\n",
    "    \n",
    "layers.append(Layer(mlp_layers[-2], mlp_layers[-1]))\n",
    "# layers.append(Activation(T.nnet.softmax))\n",
    "    \n",
    "x = T.fmatrix('x')\n",
    "t = T.fmatrix('t')\n",
    "\n",
    "params = []\n",
    "for i, layer in enumerate(layers):\n",
    "    params += layer.params\n",
    "    if i == 0:\n",
    "        layer_out = layer.f_prop(x)\n",
    "    else:\n",
    "        layer_out = layer.f_prop(layer_out)\n",
    "\n",
    "y = layers[-1].z\n",
    "# cost = T.mean(T.nnet.categorical_crossentropy(y, t))\n",
    "cost = T.mean((y - t) ** 2)\n",
    "optimizer = Adam(params=params)\n",
    "updates = optimizer.updates(cost)\n",
    "\n",
    "train = theano.function(inputs=[x, t], outputs=cost, updates=updates,\n",
    "                        allow_input_downcast=True, name='train')\n",
    "valid = theano.function(inputs=[x, t], outputs=[cost, T.argmax(y, axis=1)],\n",
    "                        allow_input_downcast=True, name='valid')\n",
    "test = theano.function(inputs=[x], outputs=T.argmax(y, axis=1), name='test')\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = train_X.shape[0]//batch_size\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    #train_X, train_y = shuffle(train_X, train_y)\n",
    "    for i in range(n_batches):\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "        train(train_X[start:end], train_y[start:end])\n",
    "    valid_cost, pred_y = valid(valid_X, valid_y)\n",
    "    print('EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f' %\n",
    "          (epoch + 1, valid_cost,\n",
    "           f1_score(np.argmax(valid_y, axis=1).astype('int32'),\n",
    "                    pred_y, average='macro')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
